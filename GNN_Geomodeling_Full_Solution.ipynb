{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQzih150Uh_J"
      },
      "outputs": [],
      "source": [
        "##NOTE - INPUT IS REQUIRED IN THIS SECTION\n",
        "#=============================================================================\n",
        "#Input data csv must be saved to your Google Drive, and the file path\n",
        "#and name below updated as needed, in order for this program to run.\n",
        "\n",
        "##INPUT DATA\n",
        "#============\n",
        "#Root directory (leave blank for local PC run):\n",
        "file_path = ''#'/content/drive/My Drive/Colab Notebooks/GNN_Geomodeling/'\n",
        "#Input drillhole training data (each is optional, but must at least provide an empty file with headers):\n",
        "input_rock_unit_filename = 'Input_Data/Folded_Rock_Unit.csv'#'Input_Data/Simp_Drillhole_data.csv'#\n",
        "input_geologic_contact_filename = 'Input_Data/Folded_Geologic_Contacts.csv'\n",
        "input_orientations_filename = 'Input_Data/Folded_Orientations.csv'\n",
        "#'Input_Data/MSOP_drillhole_data.csv'\n",
        "#'Input_Data/GeoLogic_int_drillhole_data.csv'\n",
        "\n",
        "#Input data attributes (Column names)\n",
        "#X,Y,Z must have the same name in all 3 input files\n",
        "input_name_x = \"X\"\n",
        "input_name_y = \"Y\"\n",
        "input_name_z = \"Z\"\n",
        "#rock unit:\n",
        "input_name_rock_unit = \"RockUnit\"\n",
        "#geologic contacts:\n",
        "input_name_geologic_contact = \"FieldValue\"\n",
        "#orientations:\n",
        "input_name_x_vec = \"XVec\"\n",
        "input_name_y_vec = \"YVec\"\n",
        "input_name_z_vec = \"ZVec\"\n",
        "\n",
        "##MESH SETTINGS\n",
        "#===============\n",
        "#Mesh extents and size in X,Y,Z (if less than the above dataset extents, data will be filtered)\n",
        "min_extents = [1000,2300,1690]#[6200,7000,1400]##[2400,5100,2100]\n",
        "max_extents = [1100,2480,1770]#[6800,7400,1700]##[3200,5600,2700]\n",
        "max_volume = 50#1000# #max volume of each tetrahedra in the tetrahedral mesh\n",
        "\n",
        "#Set x-slice for mesh/graph visualization on 3D plots\n",
        "min_viewing_x = 1040#6725#2825#\n",
        "max_viewing_x = 1050#6745#2845#\n",
        "\n",
        "##NEURAL NETWORK MODEL SETTINGS\n",
        "#==============================\n",
        "mUseGPUIfAvailable = True\n",
        "# Neural network parameters\n",
        "mNumEpoch = 800\n",
        "mNumHiddenLayerNeuron = 128\n",
        "# Relative weighting of the three input types in the loss function, does not need to sum to 1:\n",
        "mRockUnitWeightFactor = 0.2\n",
        "mGeologicContactWeightFactor = 0.2\n",
        "mOrientationsWeightFactor = 0.6\n",
        "# Proportions of data for training vs validation dataset, should sum to 1:\n",
        "mTrainingPercent = 0.85\n",
        "mValidationPercent = 0.15\n",
        "\n",
        "##OUTPUT SETTINGS\n",
        "#=================\n",
        "#Optionally save output - Set flags for whether to save graph and/or model\n",
        "save_GNN_graph = False\n",
        "save_GNN_graph_filename = 'Saved_Output/GeoLogic_pytorch_geometric_data.pth'\n",
        "#'Saved_Output/MSOP_pytorch_geometric_data.pth'\n",
        "save_model = False\n",
        "save_model_filename = 'Saved_Output/GeoLogic_pytorch_geometric_model.pth'\n",
        "#'Saved_Output/MSOP_pytorch_geometric_model.pth'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffjCzvDtvZ_F"
      },
      "source": [
        "# **Configure & install dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErL-uWxkX8hB",
        "outputId": "24e3edee-5816-411c-d2bb-fb16ba06af9d"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvCVg8i9qlOv",
        "outputId": "4ca1d55c-93e1-4d6f-c38d-cfd9468b6d3b"
      },
      "outputs": [],
      "source": [
        "# We assume that PyTorch is already installed\n",
        "import torch\n",
        "#torchversion = torch.__version__\n",
        "\n",
        "# Install PyTorch Scatter, PyTorch Sparse, and PyTorch Geometric\n",
        "#!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
        "#!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
        "#!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "#!pip install -q meshpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and mUseGPUIfAvailable) else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szo7Eqvet81l"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
        "from meshpy.tet import MeshInfo, build\n",
        "from meshpy.geometry import GeometryBuilder, Marker, make_box\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import math\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from scipy.spatial import cKDTree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do7xRZyRvf-f"
      },
      "source": [
        "# **Mesh generation**\n",
        "___________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUgbeaR5EiGS"
      },
      "source": [
        "**Meshing Helper Functions:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n0bFOx1vDrn"
      },
      "outputs": [],
      "source": [
        "#Function to generate edge list from tetrahedra\n",
        "def generate_edges_from_ele(element_array):\n",
        "    edges = set()\n",
        "    for element in element_array:\n",
        "        element = sorted(element)\n",
        "        for i in range(len(element)):\n",
        "            for j in range(i+1,len(element)):\n",
        "                edge = tuple([element[i],element[j]])\n",
        "                edges.add(edge)\n",
        "    return np.array(list(edges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjxAzzRPAqjZ"
      },
      "outputs": [],
      "source": [
        "def edge_length_stats(edges,nodes):\n",
        "  edge_lengths = []\n",
        "  for edge in edges:\n",
        "    # Calculate the Euclidean distance between the nodes\n",
        "    length = np.linalg.norm(nodes[edge[0]] - nodes[edge[1]])\n",
        "    # Append the edge length to the list\n",
        "    edge_lengths.append(length)\n",
        "\n",
        "  # Calculate min, max, and mean edge lengths\n",
        "  min_length = np.min(edge_lengths)\n",
        "  max_length = np.max(edge_lengths)\n",
        "  mean_length = np.mean(edge_lengths)\n",
        "\n",
        "  #print\n",
        "  print(\"min edge length: %.3f\" % min_length)\n",
        "  print(\"max edge length: %.3f\" % max_length)\n",
        "  print(\"mean edge length: %.3f\" % mean_length)\n",
        "  return edge_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfZ_udQZDrG8"
      },
      "outputs": [],
      "source": [
        "#plot histogram\n",
        "def histo(data):\n",
        "  # Plotting a histogram\n",
        "  plt.hist(data, bins=30, edgecolor='black')  # Adjust the number of bins as needed\n",
        "  plt.title('Histogram')\n",
        "  plt.xlabel('Edge Length')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwjmbFJhwQRK"
      },
      "outputs": [],
      "source": [
        "def visualize(mesh_points,mesh_edges):\n",
        "  # Access the mesh points and elements\n",
        "  mesh_points = np.array(mesh.points)\n",
        "\n",
        "  # Visualize the tetrahedral mesh using matplotlib\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "  # Plot the mesh edges\n",
        "  edge_points = [(mesh_points[edge[0]], mesh_points[edge[1]]) for edge in mesh_edges]\n",
        "  edge_collection = Line3DCollection(edge_points, color='g')  # You can adjust the color here\n",
        "  ax.add_collection3d(edge_collection)\n",
        "\n",
        "  # Plot the mesh points\n",
        "  ax.scatter(mesh_points[:, 0], mesh_points[:, 1], mesh_points[:, 2], color='r', marker='o')\n",
        "\n",
        "  # Set the extents of the plot (change the values accordingly)\n",
        "  ax.set_xlim(min_extents[0], max_extents[0])\n",
        "  ax.set_ylim(min_extents[1], max_extents[1])\n",
        "  ax.set_zlim(min_extents[2], max_extents[2])\n",
        "\n",
        "  ax.set_xlabel('X')\n",
        "  ax.set_ylabel('Y')\n",
        "  ax.set_zlabel('Z')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUvqvmj7EkeT"
      },
      "source": [
        "**Meshing main:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rNTt4dzu2J2"
      },
      "outputs": [],
      "source": [
        "points, facets, _, facet_markers = make_box(min_extents, max_extents)\n",
        "\n",
        "mesh_info = MeshInfo()\n",
        "mesh_info.set_points(points)\n",
        "mesh_info.set_facets(facets)\n",
        "\n",
        "mesh = build(mesh_info, max_volume=max_volume,volume_constraints=True, attributes=False, insert_points=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Ygaaz1Liu71E",
        "outputId": "546b9aa4-f6c3-4121-b72d-31b2164c3ec7"
      },
      "outputs": [],
      "source": [
        "#retrieve true edge list and run stats\n",
        "mesh_faces = np.array(mesh.faces)\n",
        "mesh_elements = np.array(mesh.elements)\n",
        "mesh_edges = generate_edges_from_ele(mesh_elements)\n",
        "mesh_points = np.array(mesh.points)\n",
        "edge_lengths = edge_length_stats(mesh_edges,mesh_points)\n",
        "histo(edge_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgqllc1WvQ-j",
        "outputId": "fc9f3fb0-413f-43cc-e523-4bb9a14831cb"
      },
      "outputs": [],
      "source": [
        "#print initial mesh\n",
        "print(\"%d points\" % len(mesh_points))\n",
        "print(\"%d edges\" % len(mesh_edges))\n",
        "print(\"%d tetrahedra\" % len(mesh_elements))\n",
        "print(\"%d exterior faces\" % len(mesh_faces))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r1tR8Gqv7N2"
      },
      "source": [
        "# **Import Geo Data**\n",
        "_______________\n",
        "3 types of input data: rock unit, geologic contacts, and orientations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1-v5E1OHHp3"
      },
      "outputs": [],
      "source": [
        "#import input data\n",
        "df_rock_unit = pd.read_csv(file_path + input_rock_unit_filename)\n",
        "df_geologic_contacts = pd.read_csv(file_path + input_geologic_contact_filename)\n",
        "df_orientations = pd.read_csv(file_path + input_orientations_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJs4NxE2HUXV",
        "outputId": "120ee079-45f5-4696-e645-3c5288a0e48c"
      },
      "outputs": [],
      "source": [
        "#Preview input data\n",
        "print(\"ROCK UNIT DATA PREVIEW:\")\n",
        "print(df_rock_unit.head(10))\n",
        "print(\"GEOLOGIC CONTACTS DATA PREVIEW:\")\n",
        "print(df_geologic_contacts.head(10))\n",
        "print(\"ORIENTATIONS DATA PREVIEW:\")\n",
        "print(df_orientations.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9J73wvN9SPS"
      },
      "outputs": [],
      "source": [
        "#Accepts Pandas dataframe and filters based on max ans min x,y,z\n",
        "def FilterInputData(df):\n",
        "  return df[\n",
        "    (df[input_name_x].between(min_extents[0], max_extents[0])) &\n",
        "    (df[input_name_y].between(min_extents[1], max_extents[1])) &\n",
        "    (df[input_name_z].between(min_extents[2], max_extents[2]))\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcHFQ9zrLPVu",
        "outputId": "246b78bb-ecde-4ef5-904b-2394713c69b1"
      },
      "outputs": [],
      "source": [
        "# reduce imported data based on the specified extents\n",
        "df_rock_unit = FilterInputData(df_rock_unit)\n",
        "print(\"%d input rock unit data points within specified limits\" % len(df_rock_unit))\n",
        "df_geologic_contacts = FilterInputData(df_geologic_contacts)\n",
        "print(\"%d input geologic contact measurements within specified limits\" % len(df_geologic_contacts))\n",
        "df_orientations = FilterInputData(df_orientations)\n",
        "print(\"%d input orientation measurements within specified limits\" % len(df_orientations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfuow-pTeOyg"
      },
      "source": [
        "# **Visualize Drillhole Data with Mesh**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kamUx429pWA5"
      },
      "outputs": [],
      "source": [
        "#plot interactively with an optional mesh slice\n",
        "def interactive_visualize(rock_unit_data, geologic_contact_data=None, orientation_data=None, mesh_points=None, mesh_edges=None, predicted_labels=None):\n",
        "\n",
        "  #Plot rock unit data\n",
        "  fig = px.scatter_3d(rock_unit_data, x=input_name_x, y=input_name_y, z=input_name_z, color=input_name_rock_unit)\n",
        "\n",
        "  #Plot geologic contact data\n",
        "  if geologic_contact_data is not None:\n",
        "    geologic_contact_trace = px.scatter_3d(geologic_contact_data, x=input_name_x, y=input_name_y, z=input_name_z, color=input_name_geologic_contact).data[0]\n",
        "    geologic_contact_trace.marker.symbol = 'x'  # Change marker symbol to 'x'\n",
        "    geologic_contact_trace.marker.size = 4\n",
        "    fig.add_trace(geologic_contact_trace)\n",
        "\n",
        "  #Plot orientation measurement data\n",
        "  if orientation_data is not None:\n",
        "    orientation_trace = px.scatter_3d(orientation_data, x=input_name_x, y=input_name_y, z=input_name_z).data[0]\n",
        "    orientation_trace.marker.symbol = 'diamond-open'\n",
        "    orientation_trace.marker.size = 4\n",
        "    fig.add_trace(orientation_trace)\n",
        "\n",
        "  #Plot mesh points in viewing slice\n",
        "  if mesh_points is not None:\n",
        "    if predicted_labels==None:\n",
        "      predicted_labels = np.zeros(len(mesh_points))\n",
        "    # Creating a DataFrame from the mesh points\n",
        "    df_mesh = pd.DataFrame({\n",
        "        'x': mesh_points[:, 0],\n",
        "        'y': mesh_points[:, 1],\n",
        "        'z': mesh_points[:, 2],\n",
        "        input_name_rock_unit: predicted_labels.cpu()\n",
        "\n",
        "    })\n",
        "\n",
        "    # Filter points to a slice in y-z plane\n",
        "    mesh_slice = df_mesh[(df_mesh['x'] >= min_viewing_x) & (df_mesh['x'] <= max_viewing_x)]\n",
        "\n",
        "    # Adding new points to the existing figure\n",
        "    fig.add_trace(\n",
        "        px.scatter_3d(mesh_slice, x='x', y='y', z='z', color=input_name_rock_unit).data[0]\n",
        "    )\n",
        "    fig.update_traces(marker=dict(size=5))  # Change the marker size here\n",
        "    fig.update_layout(title='Interactive 3D Plot')\n",
        "\n",
        "  #plot mesh edges in viewing slice\n",
        "  if mesh_points is not None and mesh_edges is not None:\n",
        "    for edge in mesh_edges:\n",
        "          for point in edge:\n",
        "              add_point = True\n",
        "              point_coords = mesh_points[point]\n",
        "              if (point_coords[0] < min_viewing_x or point_coords[0] > max_viewing_x):\n",
        "                  add_point = False\n",
        "                  break\n",
        "          if add_point:\n",
        "              point1, point2 = edge\n",
        "              point1 = mesh_points[point1]\n",
        "              point2 = mesh_points[point2]\n",
        "              x_vals = [point1[0],point2[0]]\n",
        "              y_vals = [point1[1],point2[1]]\n",
        "              z_vals = [point1[2],point2[2]]\n",
        "\n",
        "              fig.add_trace(go.Scatter3d(\n",
        "                  x=x_vals, y=y_vals, z=z_vals,\n",
        "                  mode='lines',\n",
        "                  line=dict(color='grey', width=2),\n",
        "                  name=''\n",
        "              ))\n",
        "\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "q_86jFCGA79T",
        "outputId": "a312943f-faf6-405d-d2cf-302a61d64dab"
      },
      "outputs": [],
      "source": [
        "#visualize input data\n",
        "interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts, orientation_data=df_orientations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN2wYrNFmC5j"
      },
      "outputs": [],
      "source": [
        "#visualize slice of the initial mesh before adjusting for input data\n",
        "#interactive_visualize(filtered_data, mesh_points, mesh_edges)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUS6-9gqgfg"
      },
      "source": [
        "# Setup Torch Geometric Dataset Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVpB7bBgFoYr"
      },
      "outputs": [],
      "source": [
        "def euclidean_dist_sq(x1,y1,z1,x2,y2,z2):\n",
        "  return ((x2-x1)*(x2-x1)+(y2-y1)*(y2-y1)+(z2-z1)*(z2-z1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8ccDXjvRfsN"
      },
      "outputs": [],
      "source": [
        "#Method adjusts the mesh such that the closest mesh node to every input data location is adjusted to coincide with the input data location\n",
        "#Method also assigns training data and validation data using masks\n",
        "def AdjustAndLabelMesh(tree, mesh_points, input_data_locations, input_labels, labels, test_mask, train_mask):\n",
        "\n",
        "    if mValidationPercent > 0:\n",
        "        skip_train = round((mTrainingPercent + mValidationPercent) / mValidationPercent)  #assign every nth node to validation dataset instead of train dataset\n",
        "    else:\n",
        "        skip_train = sys.maxsize\n",
        "\n",
        "    labelled_nodes = np.zeros(len(mesh_points))\n",
        "\n",
        "    labelled_count = 0\n",
        "    skipped_count = 0\n",
        "    mean_adjust_dist = 0\n",
        "\n",
        "    for data_point,data_label in zip(input_data_locations, input_labels):\n",
        "        min_dist, min_index = tree.query(data_point, k=1)  # Find the nearest neighbor index and distance\n",
        "\n",
        "        if labelled_nodes[min_index] == 0:\n",
        "            # Update the original mesh_points array at min_index\n",
        "            mesh_points[min_index] = [data_point[0], data_point[1], data_point[2]]\n",
        "            labels[min_index] = data_label #assign class as label\n",
        "            labelled_nodes[min_index] = 1\n",
        "            labelled_count += 1\n",
        "            #update train and test masks\n",
        "            if labelled_count % skip_train == 0:\n",
        "                test_mask[min_index] = True\n",
        "            else:\n",
        "                train_mask[min_index] = True\n",
        "            mean_adjust_dist += min_dist\n",
        "        else:\n",
        "            skipped_count += 1\n",
        "\n",
        "    mean_adjust_dist /= labelled_count\n",
        "\n",
        "    print(\"Total number of nodes labelled: {0}\".format(labelled_count))\n",
        "\n",
        "    print(\"Total number of input points skipped: {0}\".format(skipped_count))\n",
        "    #temporary until meshing is updated to honor all input data points during creation\n",
        "\n",
        "    print(\"Mean mesh adjustment: {:.2f}\".format(mean_adjust_dist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQXUGNZjMh2r",
        "outputId": "553a51ce-e792-48b1-814d-0c0810123f7a"
      },
      "outputs": [],
      "source": [
        "#Adjust mesh and assign graph labels and masks\n",
        "\n",
        "# Build a KD-tree from mesh_points_np\n",
        "tree = cKDTree(mesh_points)\n",
        "\n",
        "#ROCK UNIT\n",
        "# Input data - convert to np arrays\n",
        "locations_rock_unit_input = df_rock_unit[[input_name_x,input_name_y,input_name_z]].values\n",
        "labels_rock_unit_input = df_rock_unit[input_name_rock_unit].values\n",
        "#Graph labels and masks\n",
        "labels_rock_unit = np.ones(len(mesh_points))*-1\n",
        "train_mask_rock_unit = np.zeros(len(mesh_points), dtype=bool)\n",
        "test_mask_rock_unit = np.zeros(len(mesh_points), dtype=bool)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Mesh adjustment for rock unit measurements:\")\n",
        "\n",
        "AdjustAndLabelMesh(tree,mesh_points,locations_rock_unit_input,labels_rock_unit_input,\n",
        "        labels_rock_unit,test_mask_rock_unit,train_mask_rock_unit)\n",
        "\n",
        "#SCALAR FIELD\n",
        "# Input data - convert to np arrays\n",
        "locations_scalar_field_input = df_geologic_contacts[[input_name_x,input_name_y,input_name_z]].values\n",
        "labels_scalar_field_input = df_geologic_contacts[input_name_geologic_contact].values\n",
        "\n",
        "#Scale scalar field labels [-1,1]\n",
        "#labels_scalar_field_input_scaled = labels_scalar_field_input #initialize in case null\n",
        "if len(labels_scalar_field_input) > 0:\n",
        "        max_scalar_field = np.max(labels_scalar_field_input)\n",
        "        min_scalar_field = np.min(labels_scalar_field_input)\n",
        "        center_scalar_field = (max_scalar_field + min_scalar_field)/2\n",
        "        labels_scalar_field_input_scaled = (labels_scalar_field_input - center_scalar_field) / (max_scalar_field - min_scalar_field) * 2\n",
        "\n",
        "#Graph labels and masks\n",
        "labels_scalar_field = np.ones(len(mesh_points))*-99\n",
        "train_mask_scalar_field = np.zeros(len(mesh_points), dtype=bool)\n",
        "test_mask_scalar_field = np.zeros(len(mesh_points), dtype=bool)\n",
        "\n",
        "if len(labels_scalar_field_input) > 0:\n",
        "        print(\"\")\n",
        "        print(\"Mesh adjustment for scalar field measurements:\")\n",
        "\n",
        "        AdjustAndLabelMesh(tree,mesh_points,locations_scalar_field_input,labels_scalar_field_input_scaled,\n",
        "                labels_scalar_field,test_mask_scalar_field,train_mask_scalar_field)\n",
        "\n",
        "#ORIENTATIONS\n",
        "# Input data - convert to np arrays\n",
        "locations_orientations_input = df_orientations[[input_name_x,input_name_y,input_name_z]].values\n",
        "labels_orientations_input = df_orientations[[input_name_x_vec,input_name_y_vec,input_name_z_vec]].values\n",
        "#Graph labels and masks\n",
        "labels_orientations = np.ones((len(mesh_points),3))*-1\n",
        "train_mask_orientations = np.zeros(len(mesh_points), dtype=bool)\n",
        "test_mask_orientations = np.zeros(len(mesh_points), dtype=bool)\n",
        "\n",
        "if len(labels_orientations_input) > 0:\n",
        "        print(\"\")\n",
        "        print(\"Mesh adjustment for orientation measurements:\")\n",
        "\n",
        "        AdjustAndLabelMesh(tree,mesh_points,locations_orientations_input,labels_orientations_input,\n",
        "                labels_orientations,test_mask_orientations,train_mask_orientations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "un8TnK2qlJas",
        "outputId": "a500a91c-8ceb-49ad-8ab8-6cc998ec8d1d"
      },
      "outputs": [],
      "source": [
        "#visualize the drillhole data and mesh after mesh adjustment\n",
        "#interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts, orientation_data=df_orientations,mesh_points=mesh_points,mesh_edges=mesh_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_2UPD9i5WW5"
      },
      "outputs": [],
      "source": [
        "#populate model vectors\n",
        "node_features_ts = torch.tensor(mesh_points, dtype = torch.float)\n",
        "edge_ts = torch.tensor(mesh_edges, dtype=torch.long).t().contiguous()\n",
        "#Rock Unit\n",
        "labels_rock_unit_ts = torch.tensor(labels_rock_unit, dtype = torch.long)\n",
        "train_mask_rock_unit_ts = torch.tensor(train_mask_rock_unit, dtype = torch.bool)\n",
        "test_mask_rock_unit_ts = torch.tensor(test_mask_rock_unit, dtype = torch.bool)\n",
        "#Scalar Field\n",
        "labels_scalar_field_ts = torch.tensor(labels_scalar_field, dtype = torch.float)\n",
        "train_mask_scalar_field_ts = torch.tensor(train_mask_scalar_field, dtype = torch.bool)\n",
        "test_mask_scalar_field_ts = torch.tensor(test_mask_scalar_field, dtype = torch.bool)\n",
        "#Orientations\n",
        "labels_orientations_ts = torch.tensor(labels_orientations)\n",
        "train_mask_orientations_ts = torch.tensor(train_mask_orientations, dtype = torch.bool)\n",
        "test_mask_orientations_ts = torch.tensor(test_mask_orientations, dtype = torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HW8hWZlHCOXO"
      },
      "outputs": [],
      "source": [
        "#scale features x,y,z\n",
        "max_extents = np.array(max_extents)\n",
        "min_extents = np.array(min_extents)\n",
        "major_extents = np.max(max_extents - min_extents)\n",
        "center = torch.tensor((max_extents + min_extents)/2)\n",
        "node_features_ts = ((node_features_ts - center) / major_extents / 2).to(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w8Y7QuoZFoc"
      },
      "outputs": [],
      "source": [
        "# Print statistics about the graph.\n",
        "def GraphSummaryStats(data):\n",
        "    print(data)\n",
        "    print('==============================================================')\n",
        "\n",
        "    print(f'Number of nodes: {data.num_nodes}')\n",
        "    print(f'Number of edges: {data.num_edges}')\n",
        "    print(f'Average node degree: {(2*data.num_edges) / data.num_nodes:.2f}')\n",
        "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.4f}')\n",
        "    print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
        "    print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
        "    print(f'Contains self-loops: {data.has_self_loops()}')\n",
        "    print(f'Is undirected: {data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHOz3vR7tVcg",
        "outputId": "4ed71a30-7d5d-41b8-d29b-4b26958a2052"
      },
      "outputs": [],
      "source": [
        "#create torch graph datasets - scalar field\n",
        "data_scalar_field = Data(x=node_features_ts, edge_index=edge_ts, y=labels_scalar_field_ts)\n",
        "data_scalar_field.train_mask = train_mask_scalar_field_ts\n",
        "data_scalar_field.val_mask = test_mask_scalar_field_ts\n",
        "GraphSummaryStats(data_scalar_field)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26Rr3s8dEBw",
        "outputId": "84e714bc-78b2-4539-83d3-4af0eab3ae3c"
      },
      "outputs": [],
      "source": [
        "#create torch graph datasets - rock unit\n",
        "data_rock_unit = Data(x=node_features_ts, edge_index=edge_ts, y=labels_rock_unit_ts) \n",
        "data_rock_unit.train_mask = train_mask_rock_unit_ts\n",
        "data_rock_unit.val_mask = test_mask_rock_unit_ts\n",
        "GraphSummaryStats(data_rock_unit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create torch graph datasets - orientations\n",
        "data_orientations = Data(x=node_features_ts, edge_index=edge_ts, y=labels_orientations_ts) \n",
        "data_orientations.train_mask = train_mask_orientations_ts\n",
        "data_orientations.val_mask = test_mask_orientations_ts\n",
        "GraphSummaryStats(data_orientations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q2k2YksZveJ"
      },
      "outputs": [],
      "source": [
        "#save to google drive\n",
        "if save_GNN_graph:\n",
        "  torch.save(data_scalar_field, file_path + save_GNN_graph_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8BqmuIjbayb"
      },
      "outputs": [],
      "source": [
        "#to load data:\n",
        "#data = torch.load('/content/drive/My Drive/Colab Notebooks/MSOP_pytorch_geometric_data.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbx_wgB0gQ1a",
        "outputId": "fe0f926d-a495-430b-d972-94c0ca50d822"
      },
      "outputs": [],
      "source": [
        "#mini-batch - NOT USING FOR NOW\n",
        "train_loader = NeighborLoader(\n",
        "    data_rock_unit, #COME BACK AND CONFIRM THIS IS CORRECT\n",
        "    num_neighbors=[5,20],\n",
        "    batch_size=16,\n",
        "    input_nodes=data_rock_unit.train_mask,\n",
        ")\n",
        "\n",
        "val_loader = NeighborLoader(\n",
        "    data_rock_unit,\n",
        "    num_neighbors=[5,20],\n",
        "    batch_size=16,\n",
        "    input_nodes=data_rock_unit.val_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define custom loss function for orientation\n",
        "# Method computes orientation of the scalar field in the graph neural network using taylor series approximation, \n",
        "# then computes loss based on the difference with the user input orientation data\n",
        "# Returns: orientation loss and average angular difference\n",
        "class CustomOrientationLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomOrientationLoss, self).__init__()\n",
        "\n",
        "    def forward(self, out_scalar_field, orientations_data, mask):\n",
        "        nodes = np.where(mask.cpu())[0]\n",
        "        edge_indexes = orientations_data.edge_index\n",
        "        loss = 0.0\n",
        "        avg_ang_diff = 0.0\n",
        "        for node in nodes:\n",
        "            # Initialize a mask for adjacent nodes in the 1-hop neighborhood\n",
        "            adjacent_nodes_mask = np.zeros(len(orientations_data.x), dtype=bool)\n",
        "            edges_with_given_node = (edge_indexes == node).any(dim=0)\n",
        "            # Extract indices of nodes that share an edge with the given node\n",
        "            adjacent_nodes_pairs = edge_indexes[:, edges_with_given_node]\n",
        "\n",
        "            for node1, node2 in adjacent_nodes_pairs.T:\n",
        "                adjacent_node = node2.item() if node1 == node else node1.item()\n",
        "                # Add adjacent nodes to the mask\n",
        "                adjacent_nodes_mask[adjacent_node] = True\n",
        "\n",
        "            #Pv: matrix of x,y,z coords of one-hop neighborhood Nv nodes relative to current node\n",
        "            Pv = (orientations_data.x[adjacent_nodes_mask] - orientations_data.x[node]).T \n",
        "            #Sv: matrix of scalar field values output from neural network for one-hop neighbor Nv nodes relative to current node\n",
        "            Sv = out_scalar_field[adjacent_nodes_mask] - out_scalar_field[node]\n",
        "            Zv = Pv @ Sv # matrix multiplication for taylor series approximation of scalar field orientation at current node u\n",
        "            Av = orientations_data.y[node] # measured orientation at current node u\n",
        "            # Compute L1 norm\n",
        "            norm_Zv = torch.norm(Zv).item()\n",
        "            norm_Av = torch.norm(Av).item()\n",
        "            # Calculate difference between predicted and measured orientation at current node\n",
        "            cos_ang_diff = torch.dot(Av.float(),Zv).item() / (norm_Av * norm_Zv)\n",
        "            ang_diff = math.degrees(math.acos(cos_ang_diff)) # Don't need, temporary for debugging\n",
        "            avg_ang_diff += ang_diff\n",
        "            # Add to incremental loss\n",
        "            loss += 1 - abs(cos_ang_diff)\n",
        "        \n",
        "        node_count = len(nodes)\n",
        "\n",
        "        if node_count > 0:\n",
        "            avg_ang_diff /= node_count\n",
        "            loss /= node_count\n",
        "\n",
        "        return loss, avg_ang_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwGyJLOKb9ZS"
      },
      "outputs": [],
      "source": [
        "#GNN\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "  \"\"\"GraphSAGE\"\"\"\n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    # part 1 of the network - scalar field regression\n",
        "    self.sage1 = SAGEConv(dim_in, dim_h)\n",
        "    self.sage2 = SAGEConv(dim_h, dim_h)\n",
        "    self.sage3 = SAGEConv(dim_h, 1) #scalar field output\n",
        "\n",
        "    # part 2 of the network - rock unit classification\n",
        "    self.sage4 = SAGEConv(1, dim_out)\n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),\n",
        "                                      lr=0.005,\n",
        "                                      weight_decay=5e-4)\n",
        "    self.dim_h = dim_h\n",
        "    self.dim_out = dim_out\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    h = self.sage1(x, edge_index)\n",
        "    #h = torch.prelu(h, torch.ones(self.dim_h))\n",
        "    h = torch.relu(h)\n",
        "    #h = F.dropout(h, p=0.3, training=self.training)\n",
        "    h = self.sage2(h, edge_index)\n",
        "    #h = torch.prelu(h, torch.ones(self.dim_h))\n",
        "    h = torch.relu(h)\n",
        "    #h = F.dropout(h, p=0.3, training=self.training)\n",
        "    h = self.sage3(h, edge_index)\n",
        "\n",
        "    out_scalar_field = h[:,0]#[:, self.dim_out-1]\n",
        "\n",
        "    h = self.sage4(h, edge_index)\n",
        "\n",
        "    out_rock_unit = F.log_softmax(h, dim=1)\n",
        "\n",
        "    return out_rock_unit, out_scalar_field\n",
        "\n",
        "  def fit(self, data_rock_unit, data_scalar_field, data_orientations, epochs):\n",
        "    criterion_rock_unit = torch.nn.CrossEntropyLoss() # rock unit\n",
        "    criterion_scalar_field = torch.nn.MSELoss() # scalar field\n",
        "    criterion_orientation = CustomOrientationLoss() # orientation\n",
        "    optimizer = self.optimizer\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    self.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_accuracy = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out_rock_unit, out_scalar_field = self(data_rock_unit.x, data_rock_unit.edge_index)  # Perform a single forward pass.\n",
        "\n",
        "        # Compute the loss solely based on the training nodes.\n",
        "        loss_scalar_field = criterion_scalar_field(out_scalar_field[data_scalar_field.train_mask], data_scalar_field.y[data_scalar_field.train_mask])  \n",
        "        if math.isnan(loss_scalar_field):\n",
        "          loss_scalar_field = 0 #handle case of no contact input data\n",
        "        acc_scalar_field = accuracy_scalar_field(out_scalar_field[data_scalar_field.train_mask],data_scalar_field.y[data_scalar_field.train_mask])\n",
        "\n",
        "        loss_rock_unit = criterion_rock_unit(out_rock_unit[data_rock_unit.train_mask], data_rock_unit.y[data_rock_unit.train_mask])\n",
        "        acc_rock_unit = accuracy_rock_unit(out_rock_unit[data_rock_unit.train_mask].argmax(dim=1),data_rock_unit.y[data_rock_unit.train_mask])\n",
        "\n",
        "        # Pass scalar field to compute orientations, return orientation loss and average angular difference\n",
        "        loss_orientation, avg_ang_diff = criterion_orientation(out_scalar_field, data_orientations, data_orientations.train_mask)\n",
        "\n",
        "        total_loss = loss_rock_unit * mRockUnitWeightFactor + loss_scalar_field * mGeologicContactWeightFactor + loss_orientation * mOrientationsWeightFactor\n",
        "        total_accuracy = acc_rock_unit + acc_scalar_field\n",
        "\n",
        "        # (loss_classification + loss_regression.backward() for performance in future, consider)\n",
        "        total_loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients\n",
        "\n",
        "        #validation\n",
        "        val_loss_scalar_field = criterion_scalar_field(out_scalar_field[data_scalar_field.val_mask], data_scalar_field.y[data_scalar_field.val_mask])\n",
        "        if math.isnan(val_loss_scalar_field):\n",
        "          val_loss_scalar_field = 0 #handle case of no contact input data\n",
        "        val_acc_scalar_field = accuracy_scalar_field(out_scalar_field[data_scalar_field.val_mask],data_scalar_field.y[data_scalar_field.val_mask])\n",
        "        val_loss_rock_unit = criterion_rock_unit(out_rock_unit[data_rock_unit.val_mask], data_rock_unit.y[data_rock_unit.val_mask])\n",
        "        val_acc_rock_unit = accuracy_rock_unit(out_rock_unit[data_rock_unit.val_mask].argmax(dim=1),data_rock_unit.y[data_rock_unit.val_mask])\n",
        "        val_loss_orientation, val_avg_ang_diff = criterion_orientation(out_scalar_field, data_orientations, data_orientations.val_mask)\n",
        "\n",
        "        val_loss = val_loss_rock_unit * mRockUnitWeightFactor + val_loss_scalar_field * mGeologicContactWeightFactor + val_loss_orientation * mOrientationsWeightFactor\n",
        "        val_acc = val_acc_rock_unit + val_acc_scalar_field\n",
        "\n",
        "        train_losses.append(total_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        # Print header\n",
        "        if epoch == 0:\n",
        "          print(f'      || TRAIN                                                                           || VALIDATION')\n",
        "          print(f'Epoch || Loss Rock Unit | Loss Scalar | Loss Ori | Acc Rock Unit | MSE Scalar | Ang Diff || Loss Rock Unit | Loss Scalar | Loss Ori | Acc Rock Unit | MSE Scalar | Ang Diff')\n",
        "\n",
        "        # Print metrics every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "          s = '%5.0f || %14.3f | %11.3f | %8.3f | %12.2f%% | %9.2f%% | %5.0fdeg || '%(epoch,loss_rock_unit,loss_scalar_field,loss_orientation,acc_rock_unit,acc_scalar_field,avg_ang_diff)\n",
        "          s += '%14.3f | %11.3f | %8.3f | %12.2f%% | %9.2f%% | %5.0fdeg'%(val_loss_rock_unit,val_loss_scalar_field,val_loss_orientation,val_acc_rock_unit,val_acc_scalar_field,val_avg_ang_diff)\n",
        "          print(s)\n",
        "\n",
        "    return train_losses,val_losses\n",
        "\n",
        "\n",
        "def accuracy_rock_unit(pred_y, y):\n",
        "    \"\"\"Calculate classification accuracy.\"\"\"\n",
        "    if len(y) == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      return ((pred_y == y).sum() / len(y)).item()\n",
        "\n",
        "def accuracy_scalar_field(pred_y, y):\n",
        "    \"\"\"Calculate regression accuracy.\"\"\"\n",
        "    if len(y) == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      range = torch.max(y) - torch.min(y)\n",
        "      MSE = (((pred_y - y)/range) ** 2).mean().item()\n",
        "      return MSE\n",
        "\n",
        "def test(model, data):\n",
        "    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n",
        "    model.eval()\n",
        "    _, out = model(data.x, data.edge_index)\n",
        "    acc = accuracy_rock_unit(out.argmax(dim=1)[data.val_mask], data.y[data.val_mask])\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, data):\n",
        "    \"\"\"Evaluate the model and return prediction results.\"\"\"\n",
        "    model.eval()\n",
        "    out1, out2 = model(data.x, data.edge_index)\n",
        "    return out1, out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = \"Price: $ %8.2f, second value: %16.3f\"% (356.08977,524354243.12324)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwTRUAPUebXk"
      },
      "source": [
        "# **Train GNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD-slYpOeYcQ",
        "outputId": "d44ae896-7015-4134-a879-032ffd27fa81"
      },
      "outputs": [],
      "source": [
        "# Create GraphSAGE\n",
        "num_features = data_rock_unit.num_features\n",
        "num_classes = torch.unique(data_rock_unit.y).size(0) # number of rock unit classes\n",
        "print(f'Number of features: {num_features}')\n",
        "print(f'Number of classes: {num_classes}')\n",
        "graphsage = GraphSAGE(num_features,mNumHiddenLayerNeuron,num_classes)\n",
        "graphsage = graphsage.to(device)\n",
        "print(graphsage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "3cnhSSfWhIav",
        "outputId": "669057b1-0dc5-4529-ca08-d5b90075fc6b"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "train_losses,val_losses = graphsage.fit(data_rock_unit.to(device), data_scalar_field.to(device), data_orientations.to(device), mNumEpoch)\n",
        "\n",
        "# Test\n",
        "#print(f'\\nGraphSAGE test accuracy: {test(graphsage, data_rock_unit)*100:.2f}%\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em1-9i4rPleF"
      },
      "outputs": [],
      "source": [
        "#OPTIONAL: Save model / load existing model to skip above steps\n",
        "\n",
        "#Save PyTorch model\n",
        "if save_model:\n",
        "  torch.save(graphsage.state_dict(), file_path + save_model_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAClMG1wFawQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeQSZF94PyZD"
      },
      "outputs": [],
      "source": [
        "# # Create an instance of your GNN model\n",
        "# graphsage = GraphSAGE(num_features,128,num_classes)\n",
        "\n",
        "# # Load the saved state dictionary into your model\n",
        "# graphsage.load_state_dict(torch.load(file_path + save_model_filename))\n",
        "# graphsage.eval()  # Put the model in evaluation mode if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rrre-IykR6O"
      },
      "source": [
        "# **Validate & Visualize Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAGyZMmJW_gX"
      },
      "outputs": [],
      "source": [
        "#plot of training and cross validation loss\n",
        "\n",
        "# Calculate moving average of train_losses\n",
        "moving_avg_train = np.convolve(train_losses, np.ones(10)/10, mode='valid')\n",
        "moving_avg_val = np.convolve(val_losses, np.ones(10)/10, mode='valid')\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "#plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
        "plt.plot(epochs[:len(moving_avg_train)], moving_avg_train, 'b--', label='Training Moving Avg')\n",
        "#plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
        "plt.plot(epochs[:len(moving_avg_val)], moving_avg_val, 'r--', label='Validation Moving Avg')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhhwcbebCD-"
      },
      "outputs": [],
      "source": [
        "#Create confusion matrix\n",
        "\n",
        "# Lists to store ground truth labels and predictions\n",
        "list_labels = []\n",
        "list_predictions = []\n",
        "\n",
        "out1,out2 = evaluate(graphsage,data_scalar_field)\n",
        "predicted_rock_unit = out1.argmax(dim=1)\n",
        "predicted_scalar_field = out2\n",
        "\n",
        "combined_mask = np.logical_or(data_rock_unit.train_mask.cpu(), data_rock_unit.val_mask.cpu())\n",
        "\n",
        "filtered_predictions = predicted_rock_unit[combined_mask]\n",
        "filtered_labels = data_rock_unit.y[combined_mask]\n",
        "\n",
        "list_predictions.extend(filtered_predictions.cpu().numpy())\n",
        "list_labels.extend(filtered_labels.cpu().numpy())\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(list_labels, list_predictions)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "class_labels = ['1', '2', '3']\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print a summary of predicted values by class\n",
        "predicted_counts = Counter(predicted_rock_unit.cpu().numpy())\n",
        "predicted_counts = {k: v for k, v in sorted(predicted_counts.items(), key=lambda item: item[0])}\n",
        "print(\"Predicted values summary by Min Code:\")\n",
        "for class_label, count in predicted_counts.items():\n",
        "    print(f\"Min Code {class_label}: {count} nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Interactive plot of rock unit predictions in a mesh slice\n",
        "interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts,\n",
        "                       orientation_data=df_orientations, mesh_points=mesh_points, predicted_labels=predicted_rock_unit.cpu())\n",
        "                       #mesh_edges=mesh_edges #mesh edges off for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Interactive plot of scalar field predictions in a mesh slice\n",
        "interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts,\n",
        "                      orientation_data=df_orientations, mesh_points=mesh_points, predicted_labels=predicted_scalar_field.detach()) \n",
        "                      #mesh_edges=mesh_edges #mesh edges off for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plot the input training data with the labels as larger circles and the predictions as smaller circles within\n",
        "\n",
        "filtered_predictions_cpu = filtered_predictions.cpu().detach().numpy()\n",
        "filtered_labels_cpu = filtered_labels.cpu().detach().numpy()\n",
        "mislabel_mask = filtered_labels_cpu != filtered_predictions_cpu\n",
        "\n",
        "filtered_locations = data_rock_unit.x[combined_mask].cpu().detach().numpy()\n",
        "\n",
        "fig = px.scatter_3d(x=filtered_locations[:,0], y=filtered_locations[:,1], z=filtered_locations[:,2])\n",
        "fig.update_traces(marker=dict(color=filtered_labels_cpu, size=7, line=dict(width=2, color=filtered_labels_cpu)), selector=dict(mode='markers'))\n",
        "\n",
        "smaller_size_trace = px.scatter_3d(\n",
        "    x=filtered_locations[mislabel_mask, 0],\n",
        "    y=filtered_locations[mislabel_mask, 1],\n",
        "    z=filtered_locations[mislabel_mask, 2],\n",
        "    color=filtered_predictions_cpu[mislabel_mask]\n",
        ")\n",
        "smaller_size_trace.update_traces(\n",
        "    marker=dict(size=4),  # Adjust the size as needed\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.add_trace(smaller_size_trace.data[0])\n",
        "\n",
        "fig.update_layout(title='Interactive 3D Plot')\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
