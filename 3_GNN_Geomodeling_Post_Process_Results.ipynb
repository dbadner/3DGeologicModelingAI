{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z63y0LhjQTat"
      },
      "source": [
        "# **Key Inputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQzih150Uh_J"
      },
      "outputs": [],
      "source": [
        "##NOTE - INPUT IS REQUIRED IN THIS SECTION\n",
        "#=============================================================================\n",
        "#Input data csv must be saved to your Google Drive, and the file path\n",
        "#and name below updated as needed, in order for this program to run.\n",
        "\n",
        "##INPUT DATA\n",
        "#============\n",
        "#Root directory (leave blank for local PC run):\n",
        "file_path = ''#'/content/drive/My Drive/Colab Notebooks/GNN_Geomodeling/'\n",
        "#Input drillhole training data (each is optional, but must at least provide an empty file with headers):\n",
        "input_rock_unit_filename = 'Input_Data/MSOP_drillhole_data.csv'#'Input_Data/Folded_Rock_Unit.csv'#\n",
        "input_geologic_contact_filename = 'Input_Data/Folded_Geologic_Contacts.csv'\n",
        "input_orientations_filename = 'Input_Data/Folded_Orientations.csv'\n",
        "#'Input_Data/MSOP_drillhole_data.csv'\n",
        "#'Input_Data/GeoLogic_int_drillhole_data.csv'\n",
        "\n",
        "#Input data attributes (Column names)\n",
        "#X,Y,Z must have the same name in all 3 input files\n",
        "input_name_x = \"X\"\n",
        "input_name_y = \"Y\"\n",
        "input_name_z = \"Z\"\n",
        "#rock unit:\n",
        "input_name_rock_unit = \"RockUnit\"\n",
        "#geologic contacts:\n",
        "input_name_geologic_contact = \"FieldValue\"\n",
        "#orientations:\n",
        "input_name_x_vec = \"XVec\"\n",
        "input_name_y_vec = \"YVec\"\n",
        "input_name_z_vec = \"ZVec\"\n",
        "\n",
        "##MESH SETTINGS\n",
        "#===============\n",
        "#Mesh extents and size in X,Y,Z (if less than the above dataset extents, data will be filtered)\n",
        "min_extents = [6200,7000,1400]#[1000,2300,1690]##[2400,5100,2100]\n",
        "max_extents = [6800,7400,1700]#[1100,2480,1770]##[3200,5600,2700]\n",
        "max_volume = 1000#50# #max volume of each tetrahedra in the tetrahedral mesh\n",
        "\n",
        "#Set x-slice for mesh/graph visualization on 3D plots\n",
        "min_viewing_x = 6725#1040#2825#\n",
        "max_viewing_x = 6745#1050#2845#\n",
        "\n",
        "mUseGPUIfAvailable = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffjCzvDtvZ_F"
      },
      "source": [
        "# **Configure & install dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErL-uWxkX8hB",
        "outputId": "24e3edee-5816-411c-d2bb-fb16ba06af9d"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szo7Eqvet81l"
      },
      "outputs": [],
      "source": [
        "from AML.TrainModel import GraphSAGE\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from mpl_toolkits.mplot3d.art3d import Line3DCollection\n",
        "from meshpy.tet import MeshInfo, build\n",
        "from meshpy.geometry import GeometryBuilder, Marker, make_box\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import math\n",
        "import sys\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from scipy.spatial import cKDTree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and mUseGPUIfAvailable) else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeQSZF94PyZD"
      },
      "outputs": [],
      "source": [
        "#Load data from file\n",
        "\n",
        "# Load the array back from the file\n",
        "mesh_points = np.load('AML/Intermediate_Data/mesh_points.npy')\n",
        "\n",
        "#import input data\n",
        "df_rock_unit = pd.read_csv(file_path + input_rock_unit_filename)\n",
        "df_geologic_contacts = pd.read_csv(file_path + input_geologic_contact_filename)\n",
        "df_orientations = pd.read_csv(file_path + input_orientations_filename)\n",
        "\n",
        "# Load prepared input Data from file\n",
        "data_rock_unit = torch.load(file_path + 'AML/Intermediate_Data/data_rock_unit.dat').to(device)\n",
        "data_scalar_field = torch.load(file_path + 'AML/Intermediate_Data/data_scalar_field.dat').to(device)\n",
        "data_orientations = torch.load(file_path + 'AML/Intermediate_Data/data_orientations.dat').to(device)\n",
        "\n",
        "# Create an instance of your GNN model\n",
        "graphsage = GraphSAGE\n",
        "\n",
        "train_losses = torch.load(file_path + 'AML/Output_Data/train_losses.vec')\n",
        "val_losses = torch.load(file_path + 'AML/Output_Data/val_losses.vec')\n",
        "\n",
        "# Load the saved state dictionary into your model\n",
        "#graphsage.load_state_dict(torch.load(file_path + 'Output_Data/trained_model.pt'))\n",
        "graphsage = torch.load(file_path + 'AML/Output_Data/trained_model.pt').to(device)\n",
        "graphsage.eval()  # Put the model in evaluation mode if needed\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Accepts Pandas dataframe and filters based on max ans min x,y,z\n",
        "def FilterInputData(df):\n",
        "  return df[\n",
        "    (df[input_name_x].between(min_extents[0], max_extents[0])) &\n",
        "    (df[input_name_y].between(min_extents[1], max_extents[1])) &\n",
        "    (df[input_name_z].between(min_extents[2], max_extents[2]))\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reduce imported data based on the specified extents\n",
        "df_rock_unit = FilterInputData(df_rock_unit)\n",
        "print(\"%d input rock unit data points within specified limits\" % len(df_rock_unit))\n",
        "df_geologic_contacts = FilterInputData(df_geologic_contacts)\n",
        "print(\"%d input geologic contact measurements within specified limits\" % len(df_geologic_contacts))\n",
        "df_orientations = FilterInputData(df_orientations)\n",
        "print(\"%d input orientation measurements within specified limits\" % len(df_orientations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data):\n",
        "    \"\"\"Evaluate the model and return prediction results.\"\"\"\n",
        "    model.eval()\n",
        "    out1, out2 = model(data.x, data.edge_index)\n",
        "    return out1, out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rrre-IykR6O"
      },
      "source": [
        "# **Validate & Visualize Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAGyZMmJW_gX"
      },
      "outputs": [],
      "source": [
        "#plot of training and cross validation loss\n",
        "\n",
        "# Calculate moving average of train_losses\n",
        "moving_avg_train = np.convolve(train_losses, np.ones(10)/10, mode='valid')\n",
        "moving_avg_val = np.convolve(val_losses, np.ones(10)/10, mode='valid')\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "#plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
        "plt.plot(epochs[:len(moving_avg_train)], moving_avg_train, 'b--', label='Training Moving Avg')\n",
        "#plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
        "plt.plot(epochs[:len(moving_avg_val)], moving_avg_val, 'r--', label='Validation Moving Avg')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBhhwcbebCD-"
      },
      "outputs": [],
      "source": [
        "#Create confusion matrix\n",
        "\n",
        "# Lists to store ground truth labels and predictions\n",
        "list_labels = []\n",
        "list_predictions = []\n",
        "\n",
        "out1,out2 = evaluate(graphsage,data_scalar_field)\n",
        "predicted_rock_unit = out1.argmax(dim=1)\n",
        "predicted_scalar_field = out2\n",
        "\n",
        "combined_mask = np.logical_or(data_rock_unit.train_mask.cpu(), data_rock_unit.val_mask.cpu())\n",
        "\n",
        "filtered_predictions = predicted_rock_unit[combined_mask]\n",
        "filtered_labels = data_rock_unit.y[combined_mask]\n",
        "\n",
        "list_predictions.extend(filtered_predictions.cpu().numpy())\n",
        "list_labels.extend(filtered_labels.cpu().numpy())\n",
        "\n",
        "# Create a confusion matrix\n",
        "conf_matrix = confusion_matrix(list_labels, list_predictions)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "class_labels = ['1', '2', '3']\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print a summary of predicted values by class\n",
        "predicted_counts = Counter(predicted_rock_unit.cpu().numpy())\n",
        "predicted_counts = {k: v for k, v in sorted(predicted_counts.items(), key=lambda item: item[0])}\n",
        "print(\"Predicted values summary by Min Code:\")\n",
        "for class_label, count in predicted_counts.items():\n",
        "    print(f\"Min Code {class_label}: {count} nodes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot interactively with an optional mesh slice\n",
        "def interactive_visualize(rock_unit_data, geologic_contact_data=None, orientation_data=None, mesh_points=None, mesh_edges=None, predicted_labels=None):\n",
        "\n",
        "  #Plot rock unit data\n",
        "  fig = px.scatter_3d(rock_unit_data, x=input_name_x, y=input_name_y, z=input_name_z, color=input_name_rock_unit)\n",
        "\n",
        "  #Plot geologic contact data\n",
        "  if geologic_contact_data is not None:\n",
        "    geologic_contact_trace = px.scatter_3d(geologic_contact_data, x=input_name_x, y=input_name_y, z=input_name_z, color=input_name_geologic_contact).data[0]\n",
        "    geologic_contact_trace.marker.symbol = 'x'  # Change marker symbol to 'x'\n",
        "    geologic_contact_trace.marker.size = 4\n",
        "    fig.add_trace(geologic_contact_trace)\n",
        "\n",
        "  #Plot orientation measurement data\n",
        "  if orientation_data is not None:\n",
        "    orientation_trace = px.scatter_3d(orientation_data, x=input_name_x, y=input_name_y, z=input_name_z).data[0]\n",
        "    orientation_trace.marker.symbol = 'diamond-open'\n",
        "    orientation_trace.marker.size = 4\n",
        "    fig.add_trace(orientation_trace)\n",
        "\n",
        "  #Plot mesh points in viewing slice\n",
        "  if mesh_points is not None:\n",
        "    if predicted_labels==None:\n",
        "      predicted_labels = np.zeros(len(mesh_points))\n",
        "    # Creating a DataFrame from the mesh points\n",
        "    df_mesh = pd.DataFrame({\n",
        "        'x': mesh_points[:, 0],\n",
        "        'y': mesh_points[:, 1],\n",
        "        'z': mesh_points[:, 2],\n",
        "        input_name_rock_unit: predicted_labels\n",
        "    })\n",
        "\n",
        "    # Filter points to a slice in y-z plane\n",
        "    mesh_slice = df_mesh[(df_mesh['x'] >= min_viewing_x) & (df_mesh['x'] <= max_viewing_x)]\n",
        "\n",
        "    # Adding new points to the existing figure\n",
        "    fig.add_trace(\n",
        "        px.scatter_3d(mesh_slice, x='x', y='y', z='z', color=input_name_rock_unit).data[0]\n",
        "    )\n",
        "    fig.update_traces(marker=dict(size=5))  # Change the marker size here\n",
        "    fig.update_layout(title='Interactive 3D Plot')\n",
        "\n",
        "  #plot mesh edges in viewing slice\n",
        "  if mesh_points is not None and mesh_edges is not None:\n",
        "    for edge in mesh_edges:\n",
        "          for point in edge:\n",
        "              add_point = True\n",
        "              point_coords = mesh_points[point]\n",
        "              if (point_coords[0] < min_viewing_x or point_coords[0] > max_viewing_x):\n",
        "                  add_point = False\n",
        "                  break\n",
        "          if add_point:\n",
        "              point1, point2 = edge\n",
        "              point1 = mesh_points[point1]\n",
        "              point2 = mesh_points[point2]\n",
        "              x_vals = [point1[0],point2[0]]\n",
        "              y_vals = [point1[1],point2[1]]\n",
        "              z_vals = [point1[2],point2[2]]\n",
        "\n",
        "              fig.add_trace(go.Scatter3d(\n",
        "                  x=x_vals, y=y_vals, z=z_vals,\n",
        "                  mode='lines',\n",
        "                  line=dict(color='grey', width=2),\n",
        "                  name=''\n",
        "              ))\n",
        "\n",
        "  fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Interactive plot of rock unit predictions in a mesh slice\n",
        "interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts,\n",
        "                       orientation_data=df_orientations, mesh_points=mesh_points, predicted_labels=predicted_rock_unit.cpu())\n",
        "                       #mesh_edges=mesh_edges #mesh edges off for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Interactive plot of scalar field predictions in a mesh slice\n",
        "interactive_visualize(rock_unit_data=df_rock_unit, geologic_contact_data=df_geologic_contacts,\n",
        "                      orientation_data=df_orientations, mesh_points=mesh_points, predicted_labels=predicted_scalar_field.cpu().detach()) \n",
        "                      #mesh_edges=mesh_edges #mesh edges off for performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plot the input training data with the labels as larger circles and the predictions as smaller circles within\n",
        "\n",
        "filtered_predictions_cpu = filtered_predictions.cpu().detach().numpy()\n",
        "filtered_labels_cpu = filtered_labels.cpu().detach().numpy()\n",
        "mislabel_mask = filtered_labels_cpu != filtered_predictions_cpu\n",
        "\n",
        "filtered_locations = data_rock_unit.x[combined_mask].cpu().detach().numpy()\n",
        "\n",
        "fig = px.scatter_3d(x=filtered_locations[:,0], y=filtered_locations[:,1], z=filtered_locations[:,2])\n",
        "fig.update_traces(marker=dict(color=filtered_labels_cpu, size=7, line=dict(width=2, color=filtered_labels_cpu)), selector=dict(mode='markers'))\n",
        "\n",
        "smaller_size_trace = px.scatter_3d(\n",
        "    x=filtered_locations[mislabel_mask, 0],\n",
        "    y=filtered_locations[mislabel_mask, 1],\n",
        "    z=filtered_locations[mislabel_mask, 2],\n",
        "    color=filtered_predictions_cpu[mislabel_mask]\n",
        ")\n",
        "smaller_size_trace.update_traces(\n",
        "    marker=dict(size=4),  # Adjust the size as needed\n",
        "    selector=dict(mode='markers')\n",
        ")\n",
        "\n",
        "fig.add_trace(smaller_size_trace.data[0])\n",
        "\n",
        "fig.update_layout(title='Interactive 3D Plot')\n",
        "\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
